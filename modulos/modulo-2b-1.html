<!DOCTYPE html>
<html lang="pt-BR" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>M√≥dulo 2B.1: Anatomia de LLMs - Como Funcionam Por Dentro | SuperProfessores</title>
    <meta name="description" content="Entenda a arquitetura interna de Large Language Models. Domine conceitos de transformers, attention mechanisms, tokeniza√ß√£o e embeddings. Saiba como m">

    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- Tailwind Config -->
    <script>
        tailwind.config = {
            darkMode: 'class',
            theme: {
                extend: {
                    colors: {
                        primary: '#3B82F6',
                        'trilha-a': '#9b59b6',
                        'trilha-b': '#10B981',
                        'module-color': '#10B981',
                    },
                    fontFamily: {
                        sans: ['Inter', 'sans-serif'],
                    },
                }
            }
        }
    </script>

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <style>
        * {
            transition: background-color 200ms ease-in-out, border-color 200ms ease-in-out, color 200ms ease-in-out;
        }
        .preload * {
            transition: none !important;
        }
    </style>
</head>
<body class="preload bg-neutral-50 dark:bg-neutral-900 text-neutral-900 dark:text-neutral-100">

    <!-- Navigation -->
    <nav class="sticky top-0 z-50 bg-white/90 dark:bg-neutral-800/90 backdrop-blur-sm border-b border-neutral-200 dark:border-neutral-700">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex justify-between items-center h-16">
                <div class="flex items-center">
                    <a href="../index.html" class="text-2xl font-bold bg-gradient-to-r from-primary to-trilha-a bg-clip-text text-transparent">
                        üéì SuperProfessores
                    </a>
                </div>
                <div class="hidden md:flex items-center space-x-8">
                    <a href="../index.html" class="text-neutral-700 dark:text-neutral-300 hover:text-primary font-medium">In√≠cio</a>
                    <a href="../niveis/nivel-2b.html" class="text-neutral-700 dark:text-neutral-300 hover:text-primary font-medium">N√≠vel 2B</a>
                    <a href="https://github.com/inematds/SuperProf" target="_blank" class="text-neutral-700 dark:text-neutral-300 hover:text-primary font-medium">GitHub</a>
                    <button id="theme-toggle" class="p-2 rounded-lg bg-neutral-100 dark:bg-neutral-700 hover:bg-neutral-200 dark:hover:bg-neutral-600">
                        <svg id="theme-toggle-dark-icon" class="hidden w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                            <path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z"></path>
                        </svg>
                        <svg id="theme-toggle-light-icon" class="hidden w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                            <path d="M10 2a1 1 0 011 1v1a1 1 0 11-2 0V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95l.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707a1 1 0 11-1.414-1.414l.707-.707a1 1 0 011.414 0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707a1 1 0 00-1.414 1.414l.707.707zm1.414 8.486l-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" fill-rule="evenodd" clip-rule="evenodd"></path>
                        </svg>
                    </button>
                </div>
            </div>
        </div>
    </nav>

    <!-- Breadcrumb -->
    <div class="bg-white dark:bg-neutral-800 border-b border-neutral-200 dark:border-neutral-700">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-4">
            <nav class="flex text-sm" aria-label="Breadcrumb">
                <a href="../index.html" class="text-primary hover:text-blue-700">In√≠cio</a>
                <span class="mx-2 text-neutral-400">/</span>
                <a href="../niveis/nivel-2b.html" class="text-primary hover:text-blue-700">Trilha B</a><span class="mx-2 text-neutral-400">/</span><a href="../niveis/nivel-2b.html" class="text-primary hover:text-blue-700">N√≠vel 2B</a><span class="mx-2 text-neutral-400">/</span><span class="text-neutral-600 dark:text-neutral-400">M√≥dulo</span>
            </nav>
        </div>
    </div>

    <!-- Hero -->
    <section class="bg-gradient-to-r from-module-color to-purple-700 py-16">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-white">
            <span class="inline-block px-4 py-2 bg-white/20 rounded-full text-sm font-semibold mb-4">T√©cnico</span>
            <h1 class="text-4xl lg:text-5xl font-bold mb-4">M√≥dulo 2B.1: Anatomia de LLMs - Como Funcionam Por Dentro</h1>
            <p class="text-xl text-purple-100 max-w-3xl">
                Entenda a arquitetura interna de Large Language Models. Domine conceitos de transformers, attention mechanisms, tokeniza√ß√£o e embeddings. Saiba como modelos "pensam" para usar com mais efic√°cia.
            </p>
        </div>
    </section>

    <!-- Main Content -->
    <main class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12">

        <!-- Vis√£o Geral -->
        <section class="mb-12">
            <h2 class="text-3xl font-bold mb-6">üìñ Vis√£o Geral</h2>
            <div class="prose prose-lg dark:prose-invert max-w-none">
                <p class="text-lg text-neutral-700 dark:text-neutral-300 mb-4">
                    Entenda a arquitetura interna de Large Language Models. Domine conceitos de transformers, attention mechanisms, tokeniza√ß√£o e embeddings. Saiba como modelos "pensam" para usar com mais efic√°cia.
                </p>
                <p class='text-lg text-neutral-700 dark:text-neutral-300'><strong>Ao final deste m√≥dulo, voc√™ ser√° capaz de:</strong></p>
                <ul class='list-disc pl-6 space-y-2 text-neutral-700 dark:text-neutral-300'><li>Compreender arquitetura Transformer (n√£o precisa codificar)</li><li>Explicar como funciona self-attention e embeddings</li><li>Entender limita√ß√µes t√©cnicas (contexto, alucina√ß√µes, vieses)</li><li>Comparar diferentes LLMs (GPT, Claude, Gemini, LLaMA)</li><li>Tomar decis√µes informadas sobre qual modelo usar quando</li></ul>
            </div>
        </section>

        <!-- Conte√∫do Detalhado -->
        <section class="mb-12">
            <h2 class="text-3xl font-bold mb-8">üìö Conte√∫do Detalhado</h2>
            
            <div class="bg-white dark:bg-neutral-800 rounded-xl p-8 mb-6 shadow-lg border-l-4 border-module-color">
                <h3 class="text-2xl font-bold mb-4 flex items-center gap-3">
                    <span class="text-3xl">üß†</span>
                    <span>De Redes Neurais a Transformers</span>
                </h3>
                <div class="prose dark:prose-invert max-w-none">
                    <p class="mb-4">### Evolu√ß√£o Hist√≥rica:
<strong>1. Perceptron (1958) ‚Üí Redes Neurais Simples</strong>
``<code class="bg-neutral-100 dark:bg-neutral-700 px-2 py-1 rounded">
Input ‚Üí [Camada de neur√¥nios] ‚Üí Output
Limita√ß√£o: S√≥ fun√ß√µes lineares
</code>`<code class="bg-neutral-100 dark:bg-neutral-700 px-2 py-1 rounded">
<strong>2. Deep Learning (2010s) ‚Üí CNNs, RNNs</strong>
</code>`<code class="bg-neutral-100 dark:bg-neutral-700 px-2 py-1 rounded">
CNNs: Boas para imagens (convolu√ß√£o espacial)
RNNs: Boas para sequ√™ncias (mem√≥ria temporal)
Limita√ß√£o: RNNs n√£o escalam (vanishing gradient)
</code>`<code class="bg-neutral-100 dark:bg-neutral-700 px-2 py-1 rounded">
<strong>3. Attention Mechanism (2017) ‚Üí Transformers</strong>
</code>`<code class="bg-neutral-100 dark:bg-neutral-700 px-2 py-1 rounded">
"Attention is All You Need" (Vaswani et al)
Ideia: Focar em partes relevantes do input
Resultado: Escala para bilh√µes de par√¢metros
</code>``
### Por que Transformers Dominam:
‚úÖ <strong>Paraleliza√ß√£o:</strong> Processa texto todo de uma vez (vs RNN sequencial)
‚úÖ <strong>Long-range dependencies:</strong> Conecta palavras distantes no texto
‚úÖ <strong>Escalabilidade:</strong> Mais dados + mais par√¢metros = melhor performance
‚úÖ <strong>Transfer learning:</strong> Pr√©-treino geral + fine-tune espec√≠fico
---</p>
                </div>
            </div>

            <div class="bg-white dark:bg-neutral-800 rounded-xl p-8 mb-6 shadow-lg border-l-4 border-module-color">
                <h3 class="text-2xl font-bold mb-4 flex items-center gap-3">
                    <span class="text-3xl">üîç</span>
                    <span>Arquitetura Transformer (Simplificada)</span>
                </h3>
                <div class="prose dark:prose-invert max-w-none">
                    <p class="mb-4">### Componentes Principais:
``<code class="bg-neutral-100 dark:bg-neutral-700 px-2 py-1 rounded">
INPUT: "O gato est√° no telhado"
   ‚Üì
[1. TOKENIZA√á√ÉO]
   ‚Üí ["O", "gato", "est√°", "no", "tel", "hado"]
   ‚Üì
[2. EMBEDDING]
   ‚Üí Cada token vira vetor de 768-12288 dimens√µes
   ‚Üì
[3. POSITIONAL ENCODING]
   ‚Üí Adiciona informa√ß√£o de ordem (palavra 1, 2, 3...)
   ‚Üì
[4. SELF-ATTENTION] (üîë Magia acontece aqui)
   ‚Üí Cada palavra "olha" para todas as outras
   ‚Üí Identifica rela√ß√µes: "gato" se relaciona com "telhado"
   ‚Üì
[5. FEED-FORWARD]
   ‚Üí Transforma√ß√µes n√£o-lineares
   ‚Üì
[6. REPETIR 4-5] (12-96 vezes, dependendo do modelo)
   ‚Üì
[7. OUTPUT]
   ‚Üí Probabilidades para pr√≥xima palavra
   ‚Üí Ex: "O gato est√° no telhado [comendo: 0.3, dormindo: 0.5, ...]"
</code>``
---</p>
                </div>
            </div>

            <div class="bg-white dark:bg-neutral-800 rounded-xl p-8 mb-6 shadow-lg border-l-4 border-module-color">
                <h3 class="text-2xl font-bold mb-4 flex items-center gap-3">
                    <span class="text-3xl">üí°</span>
                    <span>Self-Attention: O Cora√ß√£o do Transformer</span>
                </h3>
                <div class="prose dark:prose-invert max-w-none">
                    <p class="mb-4">### Como Funciona (Analogia):
Imagine uma sala de aula onde cada aluno (palavra) pode fazer perguntas para todos os outros:
<strong>Frase:</strong> "O professor explica IA para alunos interessados"
<strong>Self-Attention calcula:</strong>
<ul class="space-y-2 list-disc pl-6">
<li>"professor" deveria prestar aten√ß√£o em: "explica" (0.8), "alunos" (0.6), "IA" (0.7)</li>
<li>"alunos" deveria prestar aten√ß√£o em: "interessados" (0.9), "professor" (0.5)</li>
<li>"interessados" deveria prestar aten√ß√£o em: "alunos" (0.95), "IA" (0.4)</li>
</ul>
<strong>Resultado:</strong> Modelo entende que "interessados" modifica "alunos", n√£o "professor"
### Matematicamente (Conceitual):
``<code class="bg-neutral-100 dark:bg-neutral-700 px-2 py-1 rounded">
Para cada palavra:
1. Query (Q): "O que eu estou procurando?"
2. Key (K): "O que eu tenho a oferecer?"
3. Value (V): "Qual informa√ß√£o eu carrego?"
Attention Score = Similarity(Q, K)
Output = Weighted sum of Values
Exemplo:
Palavra "gato" (Query) procura sujeitos de a√ß√£o
Palavra "pulou" (Key) oferece "sou um verbo"
Score alto ‚Üí "gato" presta aten√ß√£o em "pulou"
</code>``
### Multi-Head Attention:
Ao inv√©s de 1 mecanismo de aten√ß√£o, usa 8-96 em paralelo:
<ul class="space-y-2 list-disc pl-6">
<li>Head 1: Foca em sintaxe (sujeito-verbo-objeto)</li>
<li>Head 2: Foca em sem√¢ntica (significado)</li>
<li>Head 3: Foca em contexto longo</li>
<li>Head 4-8: Outros padr√µes aprendidos</li>
</ul>
<strong>Analogia:</strong> 8 especialistas analisando o mesmo texto de √¢ngulos diferentes
---</p>
                </div>
            </div>

            <div class="bg-white dark:bg-neutral-800 rounded-xl p-8 mb-6 shadow-lg border-l-4 border-module-color">
                <h3 class="text-2xl font-bold mb-4 flex items-center gap-3">
                    <span class="text-3xl">üì¶</span>
                    <span>Tokeniza√ß√£o: Quebrando Texto em Peda√ßos</span>
                </h3>
                <div class="prose dark:prose-invert max-w-none">
                    <p class="mb-4">### O que s√£o Tokens?
<strong>N√£o s√£o palavras!</strong> S√£o subunidades:
<strong>Exemplo (GPT-4):</strong>
``<code class="bg-neutral-100 dark:bg-neutral-700 px-2 py-1 rounded">
Input: "Superprofessores"
Tokens: ["Super", "prof", "ess", "ores"]
4 tokens (n√£o 1 palavra)
Input: "ChatGPT √© incr√≠vel!"
Tokens: ["Chat", "G", "PT", " √©", " in", "cr", "√≠vel", "!"]
8 tokens
</code>``
### Por que Tokenizar?
‚úÖ <strong>Efici√™ncia:</strong> Vocabul√°rio fixo (50k-100k tokens vs milh√µes de palavras)
‚úÖ <strong>Generaliza√ß√£o:</strong> Palavras novas podem ser compostas de tokens conhecidos
‚úÖ <strong>Multil√≠ngue:</strong> Mesmos tokens funcionam em m√∫ltiplas l√≠nguas
### Algoritmos Comuns:
<strong>1. Byte-Pair Encoding (BPE) - Usado por GPT</strong>
<ul class="space-y-2 list-disc pl-6">
<li>Come√ßa com caracteres</li>
<li>Mescla pares frequentes</li>
<li>Ex: "a" + "b" ‚Üí "ab" se aparecem juntos frequentemente</li>
</ul>
<strong>2. WordPiece - Usado por BERT</strong>
<ul class="space-y-2 list-disc pl-6">
<li>Similar a BPE, mas otimiza likelihood</li>
</ul>
<strong>3. SentencePiece - Usado por T5, LLaMA</strong>
<ul class="space-y-2 list-disc pl-6">
<li>Trata texto como sequ√™ncia de bytes (Unicode-aware)</li>
</ul>
### Implica√ß√µes para Educadores:
‚ö†Ô∏è <strong>Limite de tokens ‚â† Limite de palavras</strong>
<ul class="space-y-2 list-disc pl-6">
<li>GPT-4: 128k tokens ‚âà 96k palavras (portugu√™s)</li>
<li>Claude: 200k tokens ‚âà 150k palavras</li>
</ul>
‚ö†Ô∏è <strong>Palavras longas custam mais</strong>
<ul class="space-y-2 list-disc pl-6">
<li>"a" = 1 token</li>
<li>"Institucionaliza√ß√£o" = 5+ tokens</li>
</ul>
<strong>Ferramenta para Contar:</strong> https://platform.openai.com/tokenizer
---</p>
                </div>
            </div>

            <div class="bg-white dark:bg-neutral-800 rounded-xl p-8 mb-6 shadow-lg border-l-4 border-module-color">
                <h3 class="text-2xl font-bold mb-4 flex items-center gap-3">
                    <span class="text-3xl">üé®</span>
                    <span>Embeddings: Representando Significado em Vetores</span>
                </h3>
                <div class="prose dark:prose-invert max-w-none">
                    <p class="mb-4">### Conceito:
<strong>Palavra ‚Üí Vetor num√©rico de alta dimens√£o</strong>
<strong>Exemplo (simplificado para 3D):</strong>
``<code class="bg-neutral-100 dark:bg-neutral-700 px-2 py-1 rounded">
"rei"      ‚Üí [0.8, 0.3, 0.1]
"rainha"   ‚Üí [0.8, 0.3, 0.9]
"homem"    ‚Üí [0.5, 0.2, 0.1]
"mulher"   ‚Üí [0.5, 0.2, 0.9]
Matem√°tica vetorial:
rei - homem + mulher ‚âà rainha
[0.8,0.3,0.1] - [0.5,0.2,0.1] + [0.5,0.2,0.9] = [0.8,0.3,0.9]
</code>``
### Propriedades M√°gicas:
<strong>1. Similaridade Sem√¢ntica</strong>
Palavras similares t√™m vetores pr√≥ximos:
<ul class="space-y-2 list-disc pl-6">
<li>"cachorro" e "c√£o" ‚Üí Dist√¢ncia pequena</li>
<li>"cachorro" e "√°rvore" ‚Üí Dist√¢ncia grande</li>
</ul>
<strong>2. Analogias</strong>
<ul class="space-y-2 list-disc pl-6">
<li>Paris : Fran√ßa :: Berlim : ? ‚Üí Alemanha</li>
<li>Funcionam via aritm√©tica vetorial!</li>
</ul>
<strong>3. Transfer√™ncia de Contexto</strong>
<ul class="space-y-2 list-disc pl-6">
<li>"banco" (sentar) vs "banco" (financeiro)</li>
<li>Mesmo embedding muda significado por contexto</li>
</ul>
### Embeddings em LLMs:
<strong>GPT-4:</strong> 12,288 dimens√µes (cada token = vetor de 12k n√∫meros)
<strong>Claude 3:</strong> N√£o revelado (estimado 8k-16k)
<strong>Gemini:</strong> N√£o revelado
<strong>Visualiza√ß√£o:</strong> t-SNE ou PCA reduzem para 2D/3D para plotar
---</p>
                </div>
            </div>

            <div class="bg-white dark:bg-neutral-800 rounded-xl p-8 mb-6 shadow-lg border-l-4 border-module-color">
                <h3 class="text-2xl font-bold mb-4 flex items-center gap-3">
                    <span class="text-3xl">üìå</span>
                    <span>üèóÔ∏è Escala: De GPT-2 a GPT-4</span>
                </h3>
                <div class="prose dark:prose-invert max-w-none">
                    <p class="mb-4">### Evolu√ß√£o de Par√¢metros:
| Modelo | Par√¢metros | Contexto | Ano |
|--------|-----------|----------|-----|
| GPT-2 | 1.5B | 1k tokens | 2019 |
| GPT-3 | 175B | 4k tokens | 2020 |
| GPT-3.5 | 175B | 16k tokens | 2022 |
| GPT-4 | ~1.7T* | 128k tokens | 2023 |
| Claude 3 Opus | ?** | 200k tokens | 2024 |
| Gemini 1.5 | ?** | 1M tokens | 2024 |
*Estimado, OpenAI n√£o confirma
**N√£o revelado
### Lei de Escala (Scaling Laws):
<strong>Descoberta (Kaplan et al, 2020):</strong>
Performance ‚àù (Par√¢metros)^Œ± √ó (Dados)^Œ≤ √ó (Computa√ß√£o)^Œ≥
<strong>Implica√ß√£o:</strong> Modelos maiores com mais dados s√£o previsivelmente melhores
<strong>Mas... h√° limites:</strong>
<ul class="space-y-2 list-disc pl-6">
<li>üí∞ Custo: GPT-4 custou ~$100M para treinar</li>
<li>‚ö° Energia: Equivalente a 1000 lares/ano</li>
<li>üåç Dados: Internet tem limite</li>
<li>üìê Retorno diminui (modelo 10x maior ‚â† 10x melhor)</li>
</ul>
---</p>
                </div>
            </div>
            
            <div class="bg-gradient-to-r from-blue-50 to-purple-50 dark:from-blue-900/10 dark:to-purple-900/10 rounded-xl p-8 text-center border-2 border-dashed border-module-color">
                <p class="text-lg font-semibold text-neutral-700 dark:text-neutral-300 mb-2">
                    üìö Conte√∫do Completo
                </p>
                <p class="text-neutral-600 dark:text-neutral-400 mb-4">
                    Fa√ßa download do material completo em Markdown para acessar todos os t√≥picos, exemplos, prompts e atividades detalhadas.
                </p>
                <a href="../pdfs/modulo-2b-1.md" download class="inline-block px-6 py-3 bg-module-color text-white rounded-lg font-semibold hover:opacity-90 transition-colors">
                    üìÑ Baixar Material Completo (MD)
                </a>
            </div>
        </section>

        <!-- Recursos -->
        <section class="mb-12">
            <h2 class="text-3xl font-bold mb-6">üì¶ Recursos do M√≥dulo</h2>
            <div class="grid md:grid-cols-2 gap-6">
                <div class="bg-white dark:bg-neutral-800 p-6 rounded-xl shadow-lg">
                    <h3 class="font-bold mb-3 flex items-center gap-2">
                        <span>üìπ</span>
                        <span>Videoaulas</span>
                    </h3>
                    <p class="text-sm text-neutral-600 dark:text-neutral-400">
                        Aulas detalhadas sobre cada t√≥pico do m√≥dulo
                    </p>
                </div>
                <div class="bg-white dark:bg-neutral-800 p-6 rounded-xl shadow-lg">
                    <h3 class="font-bold mb-3 flex items-center gap-2">
                        <span>üí¨</span>
                        <span>Pr√°ticas</span>
                    </h3>
                    <p class="text-sm text-neutral-600 dark:text-neutral-400">
                        Atividades hands-on com projetos reais
                    </p>
                </div>
                <div class="bg-white dark:bg-neutral-800 p-6 rounded-xl shadow-lg">
                    <h3 class="font-bold mb-3 flex items-center gap-2">
                        <span>‚úÖ</span>
                        <span>Avalia√ß√£o</span>
                    </h3>
                    <p class="text-sm text-neutral-600 dark:text-neutral-400">
                        Quizzes e projetos para certifica√ß√£o
                    </p>
                </div>
                <div class="bg-white dark:bg-neutral-800 p-6 rounded-xl shadow-lg">
                    <h3 class="font-bold mb-3 flex items-center gap-2">
                        <span>üìö</span>
                        <span>Refer√™ncias</span>
                    </h3>
                    <p class="text-sm text-neutral-600 dark:text-neutral-400">
                        Materiais complementares selecionados
                    </p>
                </div>
            </div>
        </section>

        <!-- Navega√ß√£o -->
        <section class="flex justify-between items-center pt-8 border-t border-neutral-200 dark:border-neutral-700">
            <a href="../niveis/nivel-2b.html" class="px-6 py-3 bg-neutral-200 dark:bg-neutral-700 rounded-lg font-semibold hover:bg-neutral-300 dark:hover:bg-neutral-600 transition-colors">
                ‚Üê Voltar ao N√≠vel 2B
            </a>
        </section>

    </main>

    <!-- Footer -->
    <footer class="bg-neutral-900 dark:bg-black text-neutral-300 py-12 mt-16">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
            <p>&copy; 2025 SuperProfessores. Licen√ßa MIT.</p>
        </div>
    </footer>

    <script src="../js/app.js"></script>
</body>
</html>