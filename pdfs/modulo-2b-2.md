# M√≥dulo 2B.2: Fine-Tuning e RAG - Especializando LLMs

**N√≠vel 2B: T√©cnico | Carga Hor√°ria: 15 horas**

---

## üìñ Vis√£o Geral

Aprenda a adaptar LLMs para necessidades educacionais espec√≠ficas. Domine t√©cnicas de fine-tuning (ajuste fino) e RAG (Retrieval-Augmented Generation) para criar assistentes especializados, bases de conhecimento personalizadas e sistemas de Q&A institucionais.

### Objetivos:
- Entender quando usar RAG vs Fine-Tuning
- Implementar sistema RAG do zero (Python b√°sico)
- Criar embeddings de materiais educacionais
- Fazer fine-tuning de modelo open-source (opcional)
- Construir chatbot especializado em conte√∫do pr√≥prio

---

## üéØ RAG vs Fine-Tuning: Quando Usar Cada Um?

### Problema Comum:

**Cen√°rio:** Universidade quer chatbot que responde sobre regulamentos internos

**Op√ß√£o 1: Prompt Engineering** ‚ùå
- Colar regulamento no prompt
- Limite: Contexto m√°ximo (128k-200k tokens)
- Problema: Documento tem 500 p√°ginas = 500k tokens

**Op√ß√£o 2: Fine-Tuning** ‚ö†Ô∏è
- Treinar modelo nos regulamentos
- Problema: Caro ($$$), lento, n√£o atualiza f√°cil
- Risco: Modelo "memoriza" mas pode alucinar

**Op√ß√£o 3: RAG** ‚úÖ
- Busca trechos relevantes + injeta no prompt
- Barato, r√°pido, atualiz√°vel, verific√°vel
- **Solu√ß√£o ideal para 90% dos casos educacionais**

---

## üìä Tabela Comparativa

| Crit√©rio | RAG | Fine-Tuning |
|----------|-----|-------------|
| **Custo** | $ (barato) | $$$ (caro) |
| **Tempo setup** | Horas | Dias/Semanas |
| **Atualiza√ß√£o** | Imediata (add docs) | Requer re-treino |
| **Verificabilidade** | Alta (cita fonte) | Baixa (caixa-preta) |
| **Privacidade** | Boa (docs locais) | Depende (modelo onde?) |
| **Complexidade** | Baixa | Alta |
| **Casos de uso** | Q&A, busca, suporte | Estilo, formato, dom√≠nio |

### Regra Pr√°tica:

**Use RAG quando:**
‚úÖ Precisa de fontes verific√°veis
‚úÖ Conte√∫do muda frequentemente
‚úÖ Base de conhecimento grande (>100k tokens)
‚úÖ Budget limitado
‚úÖ Quer controle sobre o que modelo "sabe"

**Use Fine-Tuning quando:**
‚úÖ Precisa mudar comportamento/estilo do modelo
‚úÖ Dom√≠nio muito espec√≠fico (jarg√£o t√©cnico)
‚úÖ Dados sens√≠veis (n√£o podem ir para API externa)
‚úÖ Tem expertise t√©cnico + infraestrutura

**Use Ambos quando:**
‚úÖ Fine-tune para estilo + RAG para conhecimento
‚úÖ Exemplo: Modelo fine-tuned para falar como professor + RAG para conte√∫do de cursos

---

## üîç RAG: Retrieval-Augmented Generation

### Como Funciona (5 Passos):

```
1. INDEXA√á√ÉO (Feito 1x, ou quando docs mudam)
   ‚îî‚îÄ Quebrar documentos em chunks (peda√ßos)
   ‚îî‚îÄ Gerar embeddings para cada chunk
   ‚îî‚îÄ Armazenar em vector database

2. QUERY (Cada pergunta do usu√°rio)
   ‚îî‚îÄ Usu√°rio faz pergunta
   ‚îî‚îÄ Gerar embedding da pergunta

3. RETRIEVAL
   ‚îî‚îÄ Buscar chunks mais similares (cosine similarity)
   ‚îî‚îÄ Retornar top 5-10 chunks

4. AUGMENTATION
   ‚îî‚îÄ Montar prompt: "Baseado nestes trechos: [chunks], responda: [pergunta]"

5. GENERATION
   ‚îî‚îÄ LLM gera resposta usando chunks como contexto
```

---

## üõ†Ô∏è Implementando RAG: Passo-a-Passo

### Setup Inicial (Python + Bibliotecas)

```bash
pip install openai langchain chromadb pypdf sentence-transformers
```

**Bibliotecas:**
- `openai`: Acesso a GPT-4/GPT-3.5
- `langchain`: Framework para LLM apps
- `chromadb`: Vector database (gr√°tis, local)
- `pypdf`: Ler PDFs
- `sentence-transformers`: Gerar embeddings

---

### Passo 1: Preparar Documentos

**Exemplo:** 10 PDFs de apostilas de curso

```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Carregar PDFs
docs = []
for pdf_file in ["aula1.pdf", "aula2.pdf", ...]:
    loader = PyPDFLoader(pdf_file)
    docs.extend(loader.load())

# Quebrar em chunks (1000 chars, overlap 200)
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200  # Overlap garante contexto entre chunks
)
chunks = splitter.split_documents(docs)

print(f"Total de chunks: {len(chunks)}")
# Sa√≠da: Total de chunks: 487
```

**Por que chunk_size=1000?**
- Pequeno demais (100): Perde contexto
- Grande demais (5000): Retrieval impreciso
- 1000-1500: Sweet spot para maioria dos casos

---

### Passo 2: Gerar Embeddings e Armazenar

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# Gerar embeddings (usando OpenAI ada-002)
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

# Criar vector database
vectordb = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./db_curso"  # Salva localmente
)
vectordb.persist()

print("Database criado!")
```

**Custo:** ~$0.0001 per 1k tokens
- 487 chunks √ó 1000 chars ‚âà 487k tokens
- Custo: ~$0.05 (√∫nico)

**Alternativa Gr√°tis:** Usar `HuggingFaceEmbeddings` ao inv√©s de OpenAI

```python
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
)
```

---

### Passo 3: Fazer Perguntas (Query)

```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# Carregar database
vectordb = Chroma(
    persist_directory="./db_curso",
    embedding_function=embeddings
)

# Criar chain de Q&A
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(model="gpt-3.5-turbo"),
    retriever=vectordb.as_retriever(search_kwargs={"k": 5}),  # Top 5 chunks
    return_source_documents=True  # Retorna fontes
)

# Fazer pergunta
result = qa_chain("Qual a diferen√ßa entre RAG e Fine-Tuning?")

print("Resposta:", result['result'])
print("\nFontes:")
for doc in result['source_documents']:
    print(f"- {doc.metadata['source']} (p√°gina {doc.metadata['page']})")
```

**Output:**
```
Resposta: RAG (Retrieval-Augmented Generation) √© uma t√©cnica que busca
informa√ß√µes relevantes em uma base de dados e as injeta no contexto do prompt,
enquanto Fine-Tuning √© o processo de retreinar um modelo em dados espec√≠ficos...

Fontes:
- aula5.pdf (p√°gina 12)
- aula5.pdf (p√°gina 13)
- aula7.pdf (p√°gina 8)
```

---

## üé® Melhorando o RAG

### Problema 1: Retrieval Ruim (Chunks Irrelevantes)

**Sintoma:** LLM responde "N√£o encontrei informa√ß√£o sobre isso" mesmo tendo

**Causas:**
1. Embeddings ruins
2. Chunks muito grandes/pequenos
3. Pergunta mal formulada

**Solu√ß√µes:**

**A) Query Expansion (Expandir Pergunta)**
```python
# Antes
query = "RAG"

# Depois
query_expanded = """
RAG, Retrieval-Augmented Generation, busca sem√¢ntica,
recupera√ß√£o de informa√ß√£o, embeddings
"""
```

**B) Hybrid Search (Keyword + Semantic)**
```python
# Combinar BM25 (keyword) + embeddings (semantic)
from langchain.retrievers import BM25Retriever, EnsembleRetriever

keyword_retriever = BM25Retriever.from_documents(chunks)
semantic_retriever = vectordb.as_retriever()

ensemble = EnsembleRetriever(
    retrievers=[keyword_retriever, semantic_retriever],
    weights=[0.4, 0.6]  # 40% keyword, 60% semantic
)
```

**C) Reranking**
```python
# Buscar top 20, rerankar para top 5
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank(query, docs):
    pairs = [[query, doc.page_content] for doc in docs]
    scores = reranker.predict(pairs)
    return sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)[:5]
```

---

### Problema 2: Resposta Sem Contexto

**Sintoma:** Resposta correta, mas n√£o cita fonte ou d√° detalhes

**Solu√ß√£o:** Melhorar Prompt de S√≠ntese

```python
from langchain.prompts import PromptTemplate

template = """
Voc√™ √© um assistente educacional especializado.

Use APENAS os trechos abaixo para responder. Se n√£o souber, diga "N√£o encontrei essa informa√ß√£o nos materiais".

Contexto:
{context}

Pergunta: {question}

Instru√ß√µes:
1. Responda de forma clara e educativa
2. Cite a fonte ([Fonte: nome_arquivo, p√°gina X])
3. Se m√∫ltiplas fontes, liste todas
4. Use exemplos dos trechos quando poss√≠vel

Resposta:
"""

prompt = PromptTemplate(template=template, input_variables=["context", "question"])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt}
)
```

---

### Problema 3: Custo e Lat√™ncia

**Sintoma:** Cada query demora 3-5 segundos e custa $$

**Solu√ß√µes:**

**A) Cache de Embeddings**
```python
import shelve

cache = shelve.open("embedding_cache")

def get_embedding_cached(text):
    if text in cache:
        return cache[text]
    else:
        emb = embeddings.embed_query(text)
        cache[text] = emb
        return emb
```

**B) Usar Modelo Menor para Retrieval**
```python
# Retrieval: HuggingFace (gr√°tis, local)
# Generation: GPT-4 (pago, mas s√≥ 1x por query)

retriever_embeddings = HuggingFaceEmbeddings()  # Gr√°tis
generation_llm = OpenAI(model="gpt-4")  # Qualidade
```

**C) Batch Queries**
```python
# Se processando m√∫ltiplas perguntas, fazer em batch
queries = ["Pergunta 1", "Pergunta 2", ...]
embeddings_batch = embeddings.embed_documents(queries)  # 1 API call
```

---

## üîß Fine-Tuning: Quando e Como

### O que √© Fine-Tuning?

**Pr√©-treino:** Modelo aprende linguagem geral (Wikipedia, livros, web)
**Fine-tuning:** Modelo aprende tarefa/dom√≠nio espec√≠fico (seus dados)

### Exemplo: GPT-3 ‚Üí ChatGPT

```
GPT-3 (base): Completa texto
Input: "Professor √©"
Output: "uma profiss√£o importante que..." [neutro]

ChatGPT (fine-tuned): Conversa
Input: "Explique fotoss√≠ntese"
Output: "Claro! Fotoss√≠ntese √© o processo..." [instrutivo]

Diferen√ßa: Fine-tuned com 10k+ exemplos de conversas instrutivas
```

---

### Quando Fine-Tuning Faz Sentido (Educa√ß√£o):

**Caso 1: Corre√ß√£o Autom√°tica com Estilo Espec√≠fico**
```
Dados: 1000 reda√ß√µes + corre√ß√µes de professor espec√≠fico
Objetivo: Modelo que corrige no estilo desse professor
Resultado: Feedback personalizado em escala
```

**Caso 2: Gera√ß√£o de Quest√µes de M√∫ltipla Escolha**
```
Dados: 5000 quest√µes criadas por institui√ß√£o
Objetivo: Gerar novas quest√µes no mesmo formato/dificuldade
Resultado: Banco de quest√µes infinito
```

**Caso 3: Chatbot de Suporte Institucional**
```
Dados: 2 anos de tickets de suporte + respostas
Objetivo: Automatizar 70% das perguntas comuns
Resultado: Suporte 24/7
```

---

### Processo de Fine-Tuning (OpenAI GPT-3.5)

**Passo 1: Preparar Dados (JSONL)**

```json
{"messages": [
  {"role": "system", "content": "Voc√™ √© um corretor de reda√ß√µes do ENEM"},
  {"role": "user", "content": "Reda√ß√£o: [TEXTO]"},
  {"role": "assistant", "content": "An√°lise: [FEEDBACK DETALHADO]"}
]}
{"messages": [...]}
```

**Requisitos:**
- M√≠nimo: 10 exemplos (recomendado: 50-100)
- Formato: JSONL (1 exemplo por linha)
- Qualidade > Quantidade

**Passo 2: Upload e Treino**

```python
import openai

# Upload do arquivo
file = openai.File.create(
  file=open("treino.jsonl", "rb"),
  purpose='fine-tune'
)

# Iniciar fine-tune
job = openai.FineTuningJob.create(
  training_file=file.id,
  model="gpt-3.5-turbo"
)

# Acompanhar progresso
openai.FineTuningJob.retrieve(job.id)
```

**Tempo:** 10 min - 2h (depende do tamanho)
**Custo:** $0.008 / 1k tokens (8x mais barato que treino from scratch)

**Passo 3: Usar Modelo Fine-Tuned**

```python
completion = openai.ChatCompletion.create(
  model="ft:gpt-3.5-turbo:org:modelo_redacao:abc123",  # Seu modelo
  messages=[
    {"role": "system", "content": "Voc√™ √© um corretor de reda√ß√µes do ENEM"},
    {"role": "user", "content": "Reda√ß√£o: [NOVA REDA√á√ÉO]"}
  ]
)
```

---

## üéì Caso Pr√°tico: Chatbot de Curso

**Objetivo:** Criar assistente que responde d√∫vidas sobre seu curso

**Stack:**
- RAG para conte√∫do (apostilas, slides)
- LangChain para orquestra√ß√£o
- Streamlit para interface
- ChromaDB para vector store

### C√≥digo Completo (Simplificado):

```python
import streamlit as st
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import OpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# Setup
embeddings = HuggingFaceEmbeddings()
vectordb = Chroma(persist_directory="./db", embedding_function=embeddings)
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Chain
chain = ConversationalRetrievalChain.from_llm(
    llm=OpenAI(),
    retriever=vectordb.as_retriever(),
    memory=memory
)

# Interface
st.title("ü§ñ Assistente do Curso")
user_input = st.text_input("Sua pergunta:")

if user_input:
    response = chain({"question": user_input})
    st.write(response['answer'])
```

**Deploy:** Streamlit Cloud (gr√°tis) ou Vercel

---

## üîí Considera√ß√µes de Privacidade

### LGPD e Dados Educacionais:

**Problema:** Enviar dados de alunos para OpenAI/Anthropic

**Solu√ß√µes:**

**1. Anonimiza√ß√£o**
```python
import re

def anonimizar(texto):
    # Remove nomes
    texto = re.sub(r'\b[A-Z][a-z]+ [A-Z][a-z]+\b', '[NOME]', texto)
    # Remove emails
    texto = re.sub(r'\S+@\S+', '[EMAIL]', texto)
    # Remove CPFs
    texto = re.sub(r'\d{3}\.\d{3}\.\d{3}-\d{2}', '[CPF]', texto)
    return texto
```

**2. Modelo Local (Open-Source)**
```python
from langchain.llms import HuggingFacePipeline

# Baixar LLaMA 3 (8B) - roda em GPU consumer
llm = HuggingFacePipeline.from_model_id(
    model_id="meta-llama/Llama-3-8B-Instruct",
    device=0,  # GPU
    task="text-generation"
)

# Tudo roda local, zero API externa
```

**3. Azure OpenAI (Enterprise)**
- Dados n√£o s√£o usados para treinar
- SLA de privacidade
- Compliance com LGPD/GDPR

---

## üìä Benchmarking e Avalia√ß√£o

### Como Avaliar se RAG Est√° Funcionando?

**M√©tricas:**

**1. Retrieval Precision@K**
```
Dos K chunks retornados, quantos s√£o realmente relevantes?
Precision@5 = (Chunks relevantes retornados) / 5
```

**2. MRR (Mean Reciprocal Rank)**
```
Em que posi√ß√£o aparece o primeiro chunk relevante?
MRR = 1 / (posi√ß√£o do primeiro relevante)
```

**3. Answer Quality (Humano)**
```
Escala 1-5:
1. Incorreto
2. Parcialmente correto
3. Correto mas incompleto
4. Correto e completo
5. Excelente (+ fontes + exemplos)
```

### Conjunto de Teste:

Criar 20-50 perguntas com respostas esperadas:

```python
test_set = [
    {
        "question": "O que √© RAG?",
        "expected_answer": "Retrieval-Augmented Generation...",
        "relevant_chunks": ["aula5.pdf:p12", "aula5.pdf:p13"]
    },
    # ... mais exemplos
]

# Avaliar automaticamente
for item in test_set:
    result = qa_chain(item['question'])
    # Comparar result com expected_answer
    # Verificar se relevant_chunks foram retornados
```

---

## üì¶ Recursos do M√≥dulo

### üìπ Videoaulas (4h)
- RAG: Conceitos e arquitetura (50 min)
- Implementa√ß√£o RAG do zero (90 min)
- Fine-tuning: Quando e como (60 min)
- Casos pr√°ticos e troubleshooting (40 min)

### üí¨ Pr√°ticas (9h)
- Implementar RAG com seus documentos (4h)
- Melhorar retrieval (reranking, hybrid) (2h)
- Criar interface com Streamlit (2h)
- Avaliar e iterar (1h)

### ‚úÖ Avalia√ß√£o (2h)
- Projeto: Chatbot RAG funcional com ‚â•20 docs
- Demonstra√ß√£o: Responder 10 perguntas com fontes
- Documenta√ß√£o: Explicar escolhas t√©cnicas

---

## üìö Refer√™ncias

- **Paper:** "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Lewis et al, 2020)
- **Docs:** LangChain Documentation (python.langchain.com)
- **Curso:** DeepLearning.AI - Building Applications with Vector Databases
- **Blog:** Pinecone Learning Center (vector databases)

---

**¬© 2025 SuperProfessores | Licen√ßa MIT**
