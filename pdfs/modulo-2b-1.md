# M√≥dulo 2B.1: Anatomia de LLMs - Como Funcionam Por Dentro

**N√≠vel 2B: T√©cnico | Carga Hor√°ria: 15 horas**

---

## üìñ Vis√£o Geral

Entenda a arquitetura interna de Large Language Models. Domine conceitos de transformers, attention mechanisms, tokeniza√ß√£o e embeddings. Saiba como modelos "pensam" para usar com mais efic√°cia.

### Objetivos:
- Compreender arquitetura Transformer (n√£o precisa codificar)
- Explicar como funciona self-attention e embeddings
- Entender limita√ß√µes t√©cnicas (contexto, alucina√ß√µes, vieses)
- Comparar diferentes LLMs (GPT, Claude, Gemini, LLaMA)
- Tomar decis√µes informadas sobre qual modelo usar quando

---

## üß† De Redes Neurais a Transformers

### Evolu√ß√£o Hist√≥rica:

**1. Perceptron (1958) ‚Üí Redes Neurais Simples**
```
Input ‚Üí [Camada de neur√¥nios] ‚Üí Output
Limita√ß√£o: S√≥ fun√ß√µes lineares
```

**2. Deep Learning (2010s) ‚Üí CNNs, RNNs**
```
CNNs: Boas para imagens (convolu√ß√£o espacial)
RNNs: Boas para sequ√™ncias (mem√≥ria temporal)
Limita√ß√£o: RNNs n√£o escalam (vanishing gradient)
```

**3. Attention Mechanism (2017) ‚Üí Transformers**
```
"Attention is All You Need" (Vaswani et al)
Ideia: Focar em partes relevantes do input
Resultado: Escala para bilh√µes de par√¢metros
```

### Por que Transformers Dominam:

‚úÖ **Paraleliza√ß√£o:** Processa texto todo de uma vez (vs RNN sequencial)
‚úÖ **Long-range dependencies:** Conecta palavras distantes no texto
‚úÖ **Escalabilidade:** Mais dados + mais par√¢metros = melhor performance
‚úÖ **Transfer learning:** Pr√©-treino geral + fine-tune espec√≠fico

---

## üîç Arquitetura Transformer (Simplificada)

### Componentes Principais:

```
INPUT: "O gato est√° no telhado"
   ‚Üì
[1. TOKENIZA√á√ÉO]
   ‚Üí ["O", "gato", "est√°", "no", "tel", "hado"]
   ‚Üì
[2. EMBEDDING]
   ‚Üí Cada token vira vetor de 768-12288 dimens√µes
   ‚Üì
[3. POSITIONAL ENCODING]
   ‚Üí Adiciona informa√ß√£o de ordem (palavra 1, 2, 3...)
   ‚Üì
[4. SELF-ATTENTION] (üîë Magia acontece aqui)
   ‚Üí Cada palavra "olha" para todas as outras
   ‚Üí Identifica rela√ß√µes: "gato" se relaciona com "telhado"
   ‚Üì
[5. FEED-FORWARD]
   ‚Üí Transforma√ß√µes n√£o-lineares
   ‚Üì
[6. REPETIR 4-5] (12-96 vezes, dependendo do modelo)
   ‚Üì
[7. OUTPUT]
   ‚Üí Probabilidades para pr√≥xima palavra
   ‚Üí Ex: "O gato est√° no telhado [comendo: 0.3, dormindo: 0.5, ...]"
```

---

## üí° Self-Attention: O Cora√ß√£o do Transformer

### Como Funciona (Analogia):

Imagine uma sala de aula onde cada aluno (palavra) pode fazer perguntas para todos os outros:

**Frase:** "O professor explica IA para alunos interessados"

**Self-Attention calcula:**
- "professor" deveria prestar aten√ß√£o em: "explica" (0.8), "alunos" (0.6), "IA" (0.7)
- "alunos" deveria prestar aten√ß√£o em: "interessados" (0.9), "professor" (0.5)
- "interessados" deveria prestar aten√ß√£o em: "alunos" (0.95), "IA" (0.4)

**Resultado:** Modelo entende que "interessados" modifica "alunos", n√£o "professor"

### Matematicamente (Conceitual):

```
Para cada palavra:
1. Query (Q): "O que eu estou procurando?"
2. Key (K): "O que eu tenho a oferecer?"
3. Value (V): "Qual informa√ß√£o eu carrego?"

Attention Score = Similarity(Q, K)
Output = Weighted sum of Values

Exemplo:
Palavra "gato" (Query) procura sujeitos de a√ß√£o
Palavra "pulou" (Key) oferece "sou um verbo"
Score alto ‚Üí "gato" presta aten√ß√£o em "pulou"
```

### Multi-Head Attention:

Ao inv√©s de 1 mecanismo de aten√ß√£o, usa 8-96 em paralelo:
- Head 1: Foca em sintaxe (sujeito-verbo-objeto)
- Head 2: Foca em sem√¢ntica (significado)
- Head 3: Foca em contexto longo
- Head 4-8: Outros padr√µes aprendidos

**Analogia:** 8 especialistas analisando o mesmo texto de √¢ngulos diferentes

---

## üì¶ Tokeniza√ß√£o: Quebrando Texto em Peda√ßos

### O que s√£o Tokens?

**N√£o s√£o palavras!** S√£o subunidades:

**Exemplo (GPT-4):**
```
Input: "Superprofessores"
Tokens: ["Super", "prof", "ess", "ores"]
4 tokens (n√£o 1 palavra)

Input: "ChatGPT √© incr√≠vel!"
Tokens: ["Chat", "G", "PT", " √©", " in", "cr", "√≠vel", "!"]
8 tokens
```

### Por que Tokenizar?

‚úÖ **Efici√™ncia:** Vocabul√°rio fixo (50k-100k tokens vs milh√µes de palavras)
‚úÖ **Generaliza√ß√£o:** Palavras novas podem ser compostas de tokens conhecidos
‚úÖ **Multil√≠ngue:** Mesmos tokens funcionam em m√∫ltiplas l√≠nguas

### Algoritmos Comuns:

**1. Byte-Pair Encoding (BPE) - Usado por GPT**
- Come√ßa com caracteres
- Mescla pares frequentes
- Ex: "a" + "b" ‚Üí "ab" se aparecem juntos frequentemente

**2. WordPiece - Usado por BERT**
- Similar a BPE, mas otimiza likelihood

**3. SentencePiece - Usado por T5, LLaMA**
- Trata texto como sequ√™ncia de bytes (Unicode-aware)

### Implica√ß√µes para Educadores:

‚ö†Ô∏è **Limite de tokens ‚â† Limite de palavras**
- GPT-4: 128k tokens ‚âà 96k palavras (portugu√™s)
- Claude: 200k tokens ‚âà 150k palavras

‚ö†Ô∏è **Palavras longas custam mais**
- "a" = 1 token
- "Institucionaliza√ß√£o" = 5+ tokens

**Ferramenta para Contar:** https://platform.openai.com/tokenizer

---

## üé® Embeddings: Representando Significado em Vetores

### Conceito:

**Palavra ‚Üí Vetor num√©rico de alta dimens√£o**

**Exemplo (simplificado para 3D):**
```
"rei"      ‚Üí [0.8, 0.3, 0.1]
"rainha"   ‚Üí [0.8, 0.3, 0.9]
"homem"    ‚Üí [0.5, 0.2, 0.1]
"mulher"   ‚Üí [0.5, 0.2, 0.9]

Matem√°tica vetorial:
rei - homem + mulher ‚âà rainha
[0.8,0.3,0.1] - [0.5,0.2,0.1] + [0.5,0.2,0.9] = [0.8,0.3,0.9]
```

### Propriedades M√°gicas:

**1. Similaridade Sem√¢ntica**
Palavras similares t√™m vetores pr√≥ximos:
- "cachorro" e "c√£o" ‚Üí Dist√¢ncia pequena
- "cachorro" e "√°rvore" ‚Üí Dist√¢ncia grande

**2. Analogias**
- Paris : Fran√ßa :: Berlim : ? ‚Üí Alemanha
- Funcionam via aritm√©tica vetorial!

**3. Transfer√™ncia de Contexto**
- "banco" (sentar) vs "banco" (financeiro)
- Mesmo embedding muda significado por contexto

### Embeddings em LLMs:

**GPT-4:** 12,288 dimens√µes (cada token = vetor de 12k n√∫meros)
**Claude 3:** N√£o revelado (estimado 8k-16k)
**Gemini:** N√£o revelado

**Visualiza√ß√£o:** t-SNE ou PCA reduzem para 2D/3D para plotar

---

## üèóÔ∏è Escala: De GPT-2 a GPT-4

### Evolu√ß√£o de Par√¢metros:

| Modelo | Par√¢metros | Contexto | Ano |
|--------|-----------|----------|-----|
| GPT-2 | 1.5B | 1k tokens | 2019 |
| GPT-3 | 175B | 4k tokens | 2020 |
| GPT-3.5 | 175B | 16k tokens | 2022 |
| GPT-4 | ~1.7T* | 128k tokens | 2023 |
| Claude 3 Opus | ?** | 200k tokens | 2024 |
| Gemini 1.5 | ?** | 1M tokens | 2024 |

*Estimado, OpenAI n√£o confirma
**N√£o revelado

### Lei de Escala (Scaling Laws):

**Descoberta (Kaplan et al, 2020):**
Performance ‚àù (Par√¢metros)^Œ± √ó (Dados)^Œ≤ √ó (Computa√ß√£o)^Œ≥

**Implica√ß√£o:** Modelos maiores com mais dados s√£o previsivelmente melhores

**Mas... h√° limites:**
- üí∞ Custo: GPT-4 custou ~$100M para treinar
- ‚ö° Energia: Equivalente a 1000 lares/ano
- üåç Dados: Internet tem limite
- üìê Retorno diminui (modelo 10x maior ‚â† 10x melhor)

---

## üî¨ Compara√ß√£o de LLMs (Novembro 2025)

### GPT-4 (OpenAI)

**Pontos Fortes:**
‚úÖ Racioc√≠nio l√≥gico superior
‚úÖ C√≥digo (especialmente Python)
‚úÖ Seguir instru√ß√µes complexas
‚úÖ Integra√ß√£o com ferramentas (plugins)

**Pontos Fracos:**
‚ùå Contexto "apenas" 128k
‚ùå Vieses ocidentais
‚ùå Custo alto (API)

**Melhor para:** Tutoria 1-on-1, gera√ß√£o de c√≥digo, racioc√≠nio matem√°tico

---

### Claude 3 Opus (Anthropic)

**Pontos Fortes:**
‚úÖ Contexto 200k (mais longo)
‚úÖ √âtica e seguran√ßa (Constitutional AI)
‚úÖ Reda√ß√£o criativa e an√°lise liter√°ria
‚úÖ Menos alucina√ß√£o

**Pontos Fracos:**
‚ùå C√≥digo inferior ao GPT-4
‚ùå Mais "conservador" (recusa mais)

**Melhor para:** An√°lise de textos longos, feedback em reda√ß√µes, conte√∫do sens√≠vel

---

### Gemini 1.5 Pro (Google)

**Pontos Fortes:**
‚úÖ Contexto 1M tokens (absurdo!)
‚úÖ Multimodal nativo (texto+imagem+v√≠deo+√°udio)
‚úÖ Integra√ß√£o com Google Workspace
‚úÖ Busca em tempo real

**Pontos Fracos:**
‚ùå Qualidade inconsistente
‚ùå Menos controle fino

**Melhor para:** An√°lise de materiais multim√≠dia, pesquisa com fontes atuais

---

### LLaMA 3 (Meta)

**Pontos Fortes:**
‚úÖ Open-source (gratuito)
‚úÖ Pode rodar localmente (com GPU)
‚úÖ Customiz√°vel (fine-tune completo)
‚úÖ Privacidade (dados n√£o saem do servidor)

**Pontos Fracos:**
‚ùå Requer expertise t√©cnico
‚ùå Infraestrutura pr√≥pria
‚ùå Qualidade inferior aos comerciais

**Melhor para:** Institui√ß√µes com dados sens√≠veis, projetos de pesquisa, budget limitado

---

## ‚ö†Ô∏è Limita√ß√µes T√©cnicas

### 1. Limite de Contexto

**O que √©:** Quantidade m√°xima de texto que modelo "lembra"

**Implica√ß√£o:**
- Conversa longa ‚Üí Esquece in√≠cio
- Documento > contexto ‚Üí Precisa resumir/cortar

**Solu√ß√£o:**
- RAG (Retrieval Augmented Generation) - M√≥dulo 2B.2
- Summariza√ß√£o iterativa
- Sliding window

---

### 2. Alucina√ß√µes

**O que √©:** Modelo inventa fatos que parecem verdade

**Por que acontece:**
- Treinado para gerar texto plaus√≠vel, n√£o verdadeiro
- N√£o tem "check de realidade"
- Preenche lacunas com "inven√ß√µes"

**Exemplo:**
```
Prompt: "Quem ganhou Nobel de F√≠sica em 2035?"
Modelo: "Dra. Maria Santos, por trabalho em fus√£o fria"
[FALSO: 2035 √© futuro! Mas resposta parece leg√≠tima]
```

**Mitiga√ß√£o:**
- Pedir fontes e verificar
- RAG com base de conhecimento confi√°vel
- Fine-tune em dados verificados

---

### 3. Vieses

**O que √©:** Modelo reflete preconceitos nos dados de treino

**Exemplos Documentados:**
- Associa√ß√£o "CEO" ‚Üí "homem" (mais frequente que "mulher")
- Descri√ß√µes de profiss√µes variam por g√™nero
- Vieses raciais em gera√ß√£o de imagens

**Mitiga√ß√£o:**
- Estar ciente e testar
- Diversificar prompts
- Usar modelos com RLHF (Reinforcement Learning from Human Feedback)

---

### 4. Racioc√≠nio ‚â† Compreens√£o

**Modelos s√£o pattern matchers, n√£o "pensadores":**

**O que fazem bem:**
‚úÖ Reconhecer padr√µes estat√≠sticos
‚úÖ Completar sequ√™ncias
‚úÖ Interpolar conhecimento

**O que N√ÉO fazem:**
‚ùå Racioc√≠nio causal profundo
‚ùå Planejamento de longo prazo
‚ùå Compreens√£o f√≠sica do mundo
‚ùå Teoria da mente (entender inten√ß√µes)

**Exemplo de Falha:**
```
Prompt: "Tenho 3 laranjas. Como 2. Pego mais 5. Quantas tenho?"
GPT-3: "6" ‚úÖ (certo)

Prompt: "Tenho 3 laranjas. Como 2. Pego mais 5. Planto 1. Quanto tempo at√© ter mais laranjas?"
GPT-3: "Cerca de 3 anos" (correto: 3-5 anos para laranjeira dar frutos)

Prompt: "Tenho 3 laranjas. Como 2. Pego mais 5. Planto 1. Quantas laranjas tenho agora?"
GPT-3: "6" ‚ùå (errado: 5, pois plantei 1)
```

**Li√ß√£o:** Modelos falham em racioc√≠nio multi-step com mudan√ßas de estado

---

## üõ†Ô∏è Ferramentas para Explorar LLMs

### 1. Playgrounds Oficiais

**OpenAI Playground:** https://platform.openai.com/playground
- Ajustar temperatura, top-p, frequ√™ncia
- Ver tokens e custos

**Anthropic Console:** https://console.anthropic.com
- Testar Claude com controles finos

**Google AI Studio:** https://aistudio.google.com
- Testar Gemini, incluir imagens/v√≠deos

---

### 2. Visualiza√ß√µes de Attention

**BertViz:** https://github.com/jessevig/bertviz
- Visualizar o que cada attention head "olha"
- Entender decis√µes do modelo

**LLM Visualization:** https://bbycroft.net/llm
- Anima√ß√£o 3D de forward pass

---

### 3. Contadores de Tokens

**OpenAI Tokenizer:** https://platform.openai.com/tokenizer
**Hugging Face Tokenizer:** https://huggingface.co/spaces/Xenova/the-tokenizer

---

### 4. Comparadores de Modelos

**LMSYS Chatbot Arena:** https://arena.lmsys.org
- Compare respostas lado-a-lado
- Vote no melhor (crowdsourced ranking)

**Artificial Analysis:** https://artificialanalysis.ai
- Benchmarks de qualidade, velocidade, custo

---

## üì¶ Recursos do M√≥dulo

### üìπ Videoaulas (4h)
- Hist√≥ria: de RNNs a Transformers (40 min)
- Arquitetura Transformer explicada (60 min)
- Tokeniza√ß√£o e Embeddings (45 min)
- Compara√ß√£o de LLMs (55 min)

### üí¨ Pr√°ticas (9h)
- Explorar Playgrounds (3h)
- Visualizar attention (2h)
- Comparar outputs de diferentes LLMs (2h)
- Testar limites (contexto, alucina√ß√£o) (2h)

### ‚úÖ Avalia√ß√£o (2h)
- Quiz: 30 quest√µes (conceitos t√©cnicos)
- Projeto: Relat√≥rio comparando 3 LLMs para caso de uso educacional

---

## üìö Refer√™ncias

- **Paper:** "Attention is All You Need" (Vaswani et al, 2017)
- **Curso:** Stanford CS224N (NLP with Deep Learning)
- **Livro:** *Speech and Language Processing* (Jurafsky & Martin) - Cap. 10-11
- **Blog:** Jay Alammar's Illustrated Transformer (jalammar.github.io)

---

**¬© 2025 SuperProfessores | Licen√ßa MIT**
